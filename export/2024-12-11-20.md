# large language model
## Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences
- **Url**: http://arxiv.org/abs/2412.07763v1
- **Authors**: ['Alan Nawzad Amin', 'Nate Gruver', 'Yilun Kuang', 'Lily Li', 'Hunter Elliott', 'Calvin McCarter', 'Aniruddh Raghu', 'Peyton Greenside', 'Andrew Gordon Wilson']
- **Abstrat**: To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. We introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a clonal family. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody within the human immune system. We propose to guide our designs to fit previous measurements with a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic in silico experiments and designs stronger and more stable binders in in vitro wet lab experiments.





## SAT: Spatial Aptitude Training for Multimodal Language Models
- **Url**: http://arxiv.org/abs/2412.07755v1
- **Authors**: ['Arijit Ray', 'Jiafei Duan', 'Reuben Tan', 'Dina Bashkirova', 'Rose Hendrix', 'Kiana Ehsani', 'Aniruddha Kembhavi', 'Bryan A. Plummer', 'Ranjay Krishna', 'Kuo-Hao Zeng', 'Kate Saenko']
- **Abstrat**: Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23\%$ on CVBench, $8\%$ on the harder BLINK benchmark, and $18\%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available at http://arijitray1993.github.io/SAT/ .





## Beyond Retrieval: Generating Narratives in Conversational Recommender Systems
- **Url**: http://arxiv.org/abs/2410.16780v2
- **Authors**: ['Krishna Sayana', 'Raghavendra Vasudeva', 'Yuri Vasilevski', 'Kun Su', 'Liam Hebert', 'James Pine', 'Hubert Pham', 'Ambarish Jash', 'Sukhdeep Sodhi']
- **Abstrat**: The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.   First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.





## Zero-Shot ATC Coding with Large Language Models for Clinical Assessments
- **Url**: http://arxiv.org/abs/2412.07743v1
- **Authors**: ['Zijian Chen', 'John-Michael Gamble', 'Micaela Jantzi', 'John P. Hirdes', 'Jimmy Lin']
- **Abstrat**: Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to prescription records is a significant bottleneck in healthcare research and operations at Ontario Health and InterRAI Canada, requiring extensive expert time and effort. To automate this process while maintaining data privacy, we develop a practical approach using locally deployable large language models (LLMs). Inspired by recent advances in automatic International Classification of Diseases (ICD) coding, our method frames ATC coding as a hierarchical information extraction task, guiding LLMs through the ATC ontology level by level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus development on open-source Llama models suitable for privacy-sensitive deployment. Testing across Health Canada drug product data, the RABBITS benchmark, and real clinical notes from Ontario Health, our method achieves 78% exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate knowledge grounding through drug definitions, finding modest improvements in accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama 3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller models. Our results demonstrate the feasibility of automatic ATC coding in privacy-sensitive healthcare environments, providing a foundation for future deployments.





## The BrowserGym Ecosystem for Web Agent Research
- **Url**: http://arxiv.org/abs/2412.05467v2
- **Authors**: ['Thibault Le Sellier De Chezelles', 'Maxime Gasse', 'Alexandre Drouin', 'Massimo Caccia', 'Léo Boisvert', 'Megh Thakkar', 'Tom Marty', 'Rim Assouel', 'Sahar Omidi Shayegan', 'Lawrence Keunho Jang', 'Xing Han Lù', 'Ori Yoran', 'Dehan Kong', 'Frank F. Xu', 'Siva Reddy', 'Quentin Cappart', 'Graham Neubig', 'Ruslan Salakhutdinov', 'Nicolas Chapados', 'Alexandre Lacoste']
- **Abstrat**: The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic's latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models.





## MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion
- **Url**: http://arxiv.org/abs/2409.12140v2
- **Authors**: ['Sai Shashank Kalakonda', 'Shubh Maheshwari', 'Ravi Kiran Sarvadevabhatla']
- **Abstrat**: We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos are available at: https://motion-rag.github.io/





## Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing
- **Url**: http://arxiv.org/abs/2404.12253v2
- **Authors**: ['Ye Tian', 'Baolin Peng', 'Linfeng Song', 'Lifeng Jin', 'Dian Yu', 'Haitao Mi', 'Dong Yu']
- **Abstrat**: Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.





## Granite Guardian
- **Url**: http://arxiv.org/abs/2412.07724v1
- **Authors**: ['Inkit Padhi', 'Manish Nagireddy', 'Giandomenico Cornacchia', 'Subhajit Chaudhury', 'Tejaswini Pedapati', 'Pierre Dognin', 'Keerthiram Murugesan', 'Erik Miehling', 'Martín Santillán Cooper', 'Kieran Fraser', 'Giulio Zizzo', 'Muhammad Zaid Hameed', 'Mark Purcell', 'Michael Desmond', 'Qian Pan', 'Inge Vejsbjerg', 'Elizabeth M. Daly', 'Michael Hind', 'Werner Geyer', 'Ambrish Rawat', 'Kush R. Varshney', 'Prasanna Sattigeri']
- **Abstrat**: We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian




