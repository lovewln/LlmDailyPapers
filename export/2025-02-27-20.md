# large language model
## Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing
- **Url**: http://arxiv.org/abs/2502.19416v1
- **Authors**: ['Akshat Gupta', 'Christine Fang', 'Atahan Ozdemir', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli']
- **Abstrat**: This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.





## Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy
- **Url**: http://arxiv.org/abs/2412.04697v2
- **Authors**: ['Tatsuki Koga', 'Ruihan Wu', 'Kamalika Chaudhuri']
- **Abstrat**: With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$ across different models and datasets.





## Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs
- **Url**: http://arxiv.org/abs/2502.19411v1
- **Authors**: ['Dayu Yang', 'Tianyang Liu', 'Daoan Zhang', 'Antoine Simoulin', 'Xiaoyi Liu', 'Yuwei Cao', 'Zhaopu Teng', 'Xin Qian', 'Grey Yang', 'Jiebo Luo', 'Julian McAuley']
- **Abstrat**: In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.





## Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices
- **Url**: http://arxiv.org/abs/2502.19410v1
- **Authors**: ['Xinru Wang', 'Mengjie Yu', 'Hannah Nguyen', 'Michael Iuzzolino', 'Tianyi Wang', 'Peiqi Tang', 'Natasha Lynova', 'Co Tran', 'Ting Zhang', 'Naveen Sendhilnathan', 'Hrvoje Benko', 'Haijun Xia', 'Tanya Jonker']
- **Abstrat**: Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLM's explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users' time to action and cognitive load when reading an explanation. Always-on structured explanations increased users' acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.





## ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models
- **Url**: http://arxiv.org/abs/2502.19409v1
- **Authors**: ['Danae SÃ¡nchez Villegas', 'Ingo Ziegler', 'Desmond Elliott']
- **Abstrat**: Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task --achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.





## Learning Code-Edit Embedding to Model Student Debugging Behavior
- **Url**: http://arxiv.org/abs/2502.19407v1
- **Authors**: ['Hasnain Heickal', 'Andrew Lan']
- **Abstrat**: Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.





## General Reasoning Requires Learning to Reason from the Get-go
- **Url**: http://arxiv.org/abs/2502.19402v1
- **Authors**: ['Seungwook Han', 'Jyothish Pari', 'Samuel J. Gershman', 'Pulkit Agrawal']
- **Abstrat**: Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.





## TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding
- **Url**: http://arxiv.org/abs/2502.19400v1
- **Authors**: ['Max Ku', 'Thomas Chong', 'Jonathan Leung', 'Krish Shah', 'Alvin Yu', 'Wenhu Chen']
- **Abstrat**: Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.





## Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems
- **Url**: http://arxiv.org/abs/2307.08423v4
- **Authors**: ['Xuan Zhang', 'Limei Wang', 'Jacob Helwig', 'Youzhi Luo', 'Cong Fu', 'Yaochen Xie', 'Meng Liu', 'Yuchao Lin', 'Zhao Xu', 'Keqiang Yan', 'Keir Adams', 'Maurice Weiler', 'Xiner Li', 'Tianfan Fu', 'Yucheng Wang', 'Alex Strasser', 'Haiyang Yu', 'YuQing Xie', 'Xiang Fu', 'Shenglong Xu', 'Yi Liu', 'Yuanqi Du', 'Alexandra Saxton', 'Hongyi Ling', 'Hannah Lawrence', 'Hannes StÃ¤rk', 'Shurui Gui', 'Carl Edwards', 'Nicholas Gao', 'Adriana Ladera', 'Tailin Wu', 'Elyssa F. Hofgard', 'Aria Mansouri Tehrani', 'Rui Wang', 'Ameya Daigavane', 'Montgomery Bohde', 'Jerry Kurtin', 'Qian Huang', 'Tuong Phung', 'Minkai Xu', 'Chaitanya K. Joshi', 'Simon V. Mathis', 'Kamyar Azizzadenesheli', 'Ada Fang', 'AlÃ¡n Aspuru-Guzik', 'Erik Bekkers', 'Michael Bronstein', 'Marinka Zitnik', 'Anima Anandkumar', 'Stefano Ermon', 'Pietro LiÃ²', 'Rose Yu', 'Stephan GÃ¼nnemann', 'Jure Leskovec', 'Heng Ji', 'Jimeng Sun', 'Regina Barzilay', 'Tommi Jaakkola', 'Connor W. Coley', 'Xiaoning Qian', 'Xiaofeng Qian', 'Tess Smidt', 'Shuiwang Ji']
- **Abstrat**: Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.





## Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix
- **Url**: http://arxiv.org/abs/2410.11261v2
- **Authors**: ['Yingyu Liang', 'Jiangxuan Long', 'Zhenmei Shi', 'Zhao Song', 'Yufa Zhou']
- **Abstrat**: Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our empirical results demonstrate the effectiveness of our non-linear pruning approach in maintaining model performance while significantly reducing computational costs, which is beyond the current state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.





## ARCON: Advancing Auto-Regressive Continuation for Driving Videos
- **Url**: http://arxiv.org/abs/2412.03758v3
- **Authors**: ['Ruibo Ming', 'Jingwei Wu', 'Zhewei Huang', 'Zhuoxuan Ju', 'Jianming HU', 'Lihui Peng', 'Shuchang Zhou']
- **Abstrat**: Recent advancements in auto-regressive large language models (LLMs) have led to their application in video generation. This paper explores the use of Large Vision Models (LVMs) for video continuation, a task essential for building world models and predicting future frames. We introduce ARCON, a scheme that alternates between generating semantic and RGB tokens, allowing the LVM to explicitly learn high-level structural video information. We find high consistency in the RGB images and semantic maps generated without special design. Moreover, we employ an optical flow-based texture stitching method to enhance visual quality. Experiments in autonomous driving scenarios show that our model can consistently generate long videos.





## InductionBench: LLMs Fail in the Simplest Complexity Class
- **Url**: http://arxiv.org/abs/2502.15823v2
- **Authors**: ['Wenyue Hua', 'Tyler Wong', 'Sun Fei', 'Liangming Pan', 'Adam Jardine', 'William Yang Wang']
- **Abstrat**: Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.





## Stronger Models are NOT Stronger Teachers for Instruction Tuning
- **Url**: http://arxiv.org/abs/2411.07133v3
- **Authors**: ['Zhangchen Xu', 'Fengqing Jiang', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran']
- **Abstrat**: Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.




